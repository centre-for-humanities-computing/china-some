{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' VMP 2022-03-05: \n",
    "based on Analysis_RASMUS.ipynb\n",
    "some changes to make it work & some warnings.\n",
    "double-check with Nielbo. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cnsome/china_twitter/lib/python3.6/site-packages/ipykernel_launcher.py:27: MatplotlibDeprecationWarning: \n",
      "The createFontList function was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use FontManager.addfont instead.\n"
     ]
    }
   ],
   "source": [
    "#### Imports\n",
    "import pandas as pd \n",
    "import pickle as pkl \n",
    "import altair as alt\n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "# setting up matplotlib settings\n",
    "# Source: https://towardsdatascience.com/making-matplotlib-beautiful-by-default-d0d41e3534fd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "#%matplotlib inline\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import re\n",
    "import pyLDAvis\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# font\n",
    "font_dirs = ['/Library/Fonts', ]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "font_list = font_manager.createFontList(font_files)\n",
    "font_manager.fontManager.ttflist.extend(font_list)\n",
    "\n",
    "plt.rcParams['font.family'] = 'DIN Condensed Bold'\n",
    "\n",
    "# set matplotlib aesthetics\n",
    "CB91_Blue = '#2CBDFE'\n",
    "CB91_Green = '#47DBCD'\n",
    "CB91_Pink = '#F3A0F2'\n",
    "CB91_Purple = '#9D2EC5'\n",
    "CB91_Violet = '#661D98'\n",
    "CB91_Amber = '#F5B14C'\n",
    "\n",
    "color_list = [CB91_Blue, CB91_Pink, CB91_Green, CB91_Amber,\n",
    "              CB91_Purple, CB91_Violet]\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)\n",
    "\n",
    "sns.set(rc={\n",
    "            'axes.axisbelow': False,\n",
    "            'axes.edgecolor': 'lightgrey',\n",
    "            'axes.facecolor': 'None',\n",
    "            'axes.grid': False,\n",
    "            'axes.labelcolor': 'dimgrey',\n",
    "            'axes.spines.right': False,\n",
    "            'axes.spines.top': False,\n",
    "            'figure.facecolor': 'white',\n",
    "            'lines.solid_capstyle': 'round',\n",
    "            'patch.edgecolor': 'w',\n",
    "            'patch.force_edgecolor': True,\n",
    "            'text.color': 'dimgrey',\n",
    "            'xtick.bottom': False,\n",
    "            'xtick.color': 'dimgrey',\n",
    "            'xtick.direction': 'out',\n",
    "            'xtick.top': False,\n",
    "            'ytick.color': 'dimgrey',\n",
    "            'ytick.direction': 'out',\n",
    "            'ytick.left': False,\n",
    "            'ytick.right': False,\n",
    "            'savefig.dpi': 800})\n",
    "\n",
    "#plt.rcParams[\"savefig.dpi\"] = 'figure'\n",
    "sns.set_context(\"notebook\", rc={\"font.size\":12,\n",
    "                                \"axes.titlesize\":16,\n",
    "                                \"axes.labelsize\":16})\n",
    "\n",
    "\n",
    "#alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which is basically the same as in \"helper_functions\" but manages new paths \n",
    "def prep_data():\n",
    "    \"\"\"Preparing the data\n",
    "\n",
    "    Returns:\n",
    "        pd.Dataframes: Returns three dataframes - the whole dataset, the diplomats and the media.\n",
    "    \"\"\"    \n",
    "    en_df = pd.read_csv(\"../data/all_from_clean.csv\")\n",
    "\n",
    "    en_df['month'] = pd.DatetimeIndex(en_df['created_at']).month\n",
    "    en_df['date'] = pd.DatetimeIndex(en_df['created_at']).date\n",
    "    \n",
    "    \n",
    "    def retweet_binary(string):\n",
    "        if string == \"retweeted\":\n",
    "            return \"Retweet\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "\n",
    "    en_df[\"retweet_bin\"] = en_df[\"retweet\"].apply(lambda x: retweet_binary(x))\n",
    "    \n",
    "    en_df[\"date\"] = pd.to_datetime(en_df[\"date\"])\n",
    "    \n",
    "    return en_df, en_df[en_df[\"category\"] == \"Diplomat\"].reset_index(drop=True), en_df[en_df[\"category\"] == \"Media\"].reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, diplomats, media = prep_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    with open(\"../data/models/Media_LDA.pkl\", \"rb\") as f:\n",
    "        media = pkl.load(f)\n",
    "    \n",
    "    with open(\"../data/models/Diplomat_LDA.pkl\", \"rb\") as f:\n",
    "        diplo = pkl.load(f)\n",
    "        \n",
    "    return media, diplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cnsome/china_twitter/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "/home/cnsome/china_twitter/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "media_dict, diplo_dict = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diplomats without retweets\n",
    "diplo_without = diplomats[diplomats[\"retweet\"] != \"retweeted\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VMP 2022-03-05\n",
    "# for the below I have just used the code in Analysis_RASMUS.ipynb\n",
    "# not sure whether diplo_dict() should be filtered for retweets?\n",
    "# made some changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] predicting Î¸...\n",
      "*************************\n",
      "[INFO] processed 5000/25830\n",
      "[INFO] processed 10000/25830\n",
      "[INFO] processed 15000/25830\n",
      "[INFO] processed 20000/25830\n",
      "[INFO] processed 25000/25830\n",
      "maximum number of topics: 30\n",
      "*************************\n",
      "[INFO] data exported to ../data/diplomats_noretweet_theta_df.csv\n",
      "[INFO] data exported to ../data/diplomats_noretweet_theta_df.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Diplomat document representation and metadata\n",
    "## add $\\theta$ to to diplomats\n",
    "\n",
    "## comment0: diplo_without, model is trained without retweet to avoid redundancy\n",
    "## comment1: corpus is filtered for extremes with Gensim \n",
    "## comment2 (VMP 2022-03-05): using max_topics, rather than len(theta[0]). \n",
    "\n",
    "# predict theta_i on document in corpus \n",
    "print(f'\\n[INFO] predicting \\u03B8...\\n{\"*\"*25}')\n",
    "verbose = 5000\n",
    "theta = list()\n",
    "for i, doc in enumerate(diplo_dict['corpus']):\n",
    "    vector = diplo_dict['model'][doc]\n",
    "    theta.append(vector[0])\n",
    "    if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "        print(f'[INFO] processed {i + 1}/{len(diplo_dict[\"corpus\"])}')\n",
    "# dimension consistent theta array\n",
    "ntopics_lst = [len(doc) for doc in theta]\n",
    "max_topics = max(ntopics_lst)\n",
    "print(f\"maximum number of topics: {max_topics}\")\n",
    "(m, k) = (len(theta), max_topics) #kappa VMP2022-03-05: (previously: (len(theta), len(theta[0])))\n",
    "theta_transform = np.zeros((m, k))\n",
    "for (i, doc) in enumerate(theta):\n",
    "    theta_i = np.zeros((1,30)) # VMP: where is this used?\n",
    "    for j, var in doc:\n",
    "        theta_transform[i,j] = var\n",
    "# write to csv\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(theta_transform[0,:])):\n",
    "    df[f'var {i}'] = theta_transform[:,i]\n",
    "dfout = pd.concat([diplo_without, df], axis=1)\n",
    "fname = os.path.join('..', 'data', 'diplomats_noretweet_theta_df.csv')\n",
    "dfout.to_csv(fname, index=False)\n",
    "print(f'{\"*\"*25}\\n[INFO] data exported to {fname}')\n",
    "fname_maxqda = '..' + fname.split('.')[2] + '.xlsx' # VMP2022-03-05: (previously: fname.split('.')[0] + '.xlsx')\n",
    "dfout['created_at'] = dfout['created_at'].astype(str)\n",
    "dfout.to_excel(fname_maxqda, index=False)\n",
    "print(f'[INFO] data exported to {fname_maxqda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trouble-shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trouble-shoot the problem (IndexError: --> theta_transform[i,j] = var)\n",
    "print(len(theta)) # 25.830 elements\n",
    "print(len(theta[0])) # length 8 for the first element -- issue is that some elements are longer. \n",
    "print(theta_transform.shape) # and this is exactly what we specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "### find \"longest\" list in theta (topics)\n",
    "# this is the issue -- the longest list has 30 elements \n",
    "ntopics_lst = [len(doc) for doc in theta]\n",
    "max_topics = max(ntopics_lst)\n",
    "print(max_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "china_twitter",
   "language": "python",
   "name": "china_twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
